- paper: "Deep learning UI design patterns of mobile apps"
  authors: "Phong Minh Vu, Tam The Nguyen, Hung Viet Pham, Tung Thanh Nguyen"
  year: "2018"
  conference: "ICSE"
  abstract: "User interface (UI) is one of the most important components of a mobile app and strongly influences users' perception of the app. However, UI design tasks are typically manual and time-consuming. This paper proposes a novel approach to (semi)-automate those tasks. Our key idea is to develop and deploy advanced deep learning models based on recurrent neural networks (RNN) and generative adversarial networks (GAN) to learn UI design patterns from millions of currently available mobile apps. Once trained, those models can be used to search for UI design samples given user-provided descriptions written in natural language and generate professional-looking UI designs from simpler, less elegant design drafts."

- paper: "Recommending exception handling patterns with ExAssist"
  authors: "Phong Minh Vu, Tam The Nguyen, Hung Viet Pham, Tung Thanh Nguyen"
  year: "2018"
  conference: "ICSE"
  abstract: "Exception handling is an advanced programming technique to prevent run-time errors or crashes for modern software systems. However, inexperienced programmers might fail to write proper exception handling code in their programs. In this paper, we introduce ExAssist, a code recommendation tool for exception handling. Ex-Assist can predict what types of exception could occur in a given piece of code and recommend proper exception handling code for such an exception. Preliminary evaluation of ExAssist suggests that it provides highly accurate recommendations."

- paper: "ALPACA-advanced linguistic pattern and concept analysis framework for software engineering corpora"
  authors: "Phong Minh Vu, Tam The Nguyen, Hung Viet Pham, Tung Thanh Nguyen"
  year: "2018"
  conference: "ICSE"
  abstract: "Software engineering corpora often contain domain-specific concepts and linguistic patterns. Popular text analysis tools are not specially designed to analyze such concepts and patterns. In this paper, we introduce ALPACA, a novel, customizable text analysis framework. The main purpose of ALPACA is to analyze topics and their trends in a text corpus. It allows users to define a topic with a few initial domain-specific keywords and expand it into a much larger set. Every single keyword can be expanded into long clauses to describe topics more precisely. ALPACA extracts those clauses by matching text with linguistic patterns, which are long sequences mixing both specific words and part-of-speech tags frequently appeared in the corpus. ALPACA can detect these patterns directly from pre-processed text We present one example demonstrates the use of ALPACA for text corpora of security reports."

- paper: "Interaction-Based Tracking of Program Entities for Test Case Evolution"
  authors: Hoan Anh Nguyen, Tung Thanh Nguyen, Tien N. Nguyen, Hung Viet Nguyen
  year: "2017"
  conference: "ICSME"
  abstract: "After changes are made to a system, developers typically perform regression testing to uncover the regression faults in previously existing functionality of the system. However, during software evolution, the program entities (i.e., classes/methods) realizing such functionality might be modified/replaced by other entities. Thus, in the new version, existing test cases containing obsolete class references or method calls might be broken or might not test the intended functionality. To repair the broken method calls in those test cases, for each obsolete class/method, a tester needs to find the corresponding entity that provides the same/similar function or has the same role in the new version. To automate that task, we present ITRACK, a novel tool for matching program entities across versions, which mainly relies on their interactions in the code. The key idea is that the role and functionality of an entity correlate with its interactions with other entities (e.g., how it uses or is used by others). Two entities in two versions are matched based on the similarity of their interactions with other entities in the respective versions via our novel iterative matching algorithm. Our empirical evaluation shows that ITRACK achieves from 84-99% accuracy in identifying the calls in previous test cases that need to be adapted in accordance with the replacements of entities and provide such matching to support repairing broken method calls."

- paper: "Toward Mining Visual Log of Software"
  authors: "Hung Pham, Tam Nguyen, Phong Vu, Tung Nguyen"
  year: "2016"
  conference: arXiv preprint arXiv:1610.08911
  abstract: "In this paper, we define visual log of a software system as data capturing the interactions between its users and its graphic user interface (GUI), such as screen-shots and screen recordings. We vision that mining such visual log could be useful for bug reproducing and debugging, automated GUI testing, user interface designing, question answering of common usages in software support, etc. Toward that vision, we propose a core framework for mining visual log of software. This framework focuses on detecting GUI elements and changes in visual log, removing users' private data, recognizing user interactions with GUI elements, and learning GUI usage patterns. We also performed a small study on the characteristics of GUI elements in mobile apps. The findings from this study suggested several heuristics to design techniques for recognizing GUI elements and interactions."

- paper: "Phrase-based extraction of user opinions in mobile app reviews"
  authors: "Phong Minh Vu, Tam The Nguyen, Hung Viet Pham, Tung Thanh Nguyen"
  year: "2016"
  conference: "ASE"
  abstract: "Mobile app reviews often contain useful user opinions like bug reports or suggestions. However, looking for those opinions manually in thousands of reviews is inefective and time-consuming. In this paper, we propose PUMA, an automated, phrase-based approach to extract user opinions in app reviews. Our approach includes a technique to extract phrases in reviews using part-of-speech (PoS) templates; a technique to cluster phrases having similar meanings (each cluster is considered as a major user opinion); and a technique to monitor phrase clusters with negative sentiments for their outbreaks over time. We used PUMA to study two popular apps and found that it can reveal severe problems of those apps reported in their user reviews."

- paper: "Learning API usages from bytecode: a statistical approach"
  authors: "Tam The Nguyen, Hung Viet Pham, Phong Minh Vu, Tung Thanh Nguyen"
  year: "2016"
  conference: "ICSE"
  abstract: "When developing mobile apps, programmers rely heavily on standard API frameworks and libraries. However, learning and using those APIs is often challenging due to the fast-changing nature of API frameworks for mobile systems, the complexity of API usages, the insufficiency of documentation, and the unavailability of source code examples. In this paper, we propose a novel approach to learn API usages from bytecode of Android mobile apps. Our core contributions include: i) ARUS, a graph-based representation of API usage scenarios; ii) HAPI, a statistical, generative model of API usages; and iii) three algorithms to extract ARUS from apps' bytecode, to train HAPI based on method call sequences extracted from ARUS, and to recommend method calls in code completion engines using the trained HAPI. Our empirical evaluation suggests that our approach can learn useful API usage models which can provide recommendations with higher levels of accuracy than the baseline n-gram model."

- paper: "Divide-and-conquer approach for multi-phase statistical migration for source code (t)"
  authors: "Anh Tuan Nguyen, Tung Thanh Nguyen, Tien N. Nguyen"
  year: "2015"
  conference: "ASE"
  abstract: "Prior research shows that directly applying phrase-based SMT on lexical tokens to migrate Java to C produces much semantically incorrect code. A key limitation is the use of sequences in phrase-based SMT to model and translate source code with well-formed structures. We propose mppSMT, a divide-and-conquer technique to address that with novel training and migration algorithms using phrase-based SMT in three phases. First, mppSMT treats a program as a sequence of syntactic units and maps/translates such sequences in two languages to one another. Second, with a syntax-directed fashion, it deals with the tokens within syntactic units by encoding them with semantic symbols to represent their data and token types. This encoding via semantic symbols helps better migration of API usages. Third, the lexical tokens corresponding to each sememe are mapped or migrated. The resulting sequences of tokens are merged together to form the final migrated code. Such divide-and-conquer and syntax-direction strategies enable phrase-based SMT to adapt well to syntactical structures in source code, thus, improving migration accuracy. Our empirical evaluation on several real-world systems shows that 84.8 -- 97.9% and 70 -- 83% of the migrated methods are syntactically and semantically correct, respectively. 26.3 -- 51.2% of total migrated methods are exactly matched to the human-written C code in the oracle. Compared to Java2CSharp, a rule-based migration tool, it achieves higher semantic accuracy from 6.6 -- 57.7% relatively. Importantly, it does not require manual labeling for training data or manual definition of rules."

- paper: "Recommending api usages for mobile apps with hidden markov model"
  authors: "Tam The Nguyen, Hung Viet Pham, Phong Minh Vu, Tung Thanh Nguyen"
  year: "2015"
  conference: "ASE"
  abstract: "Mobile apps often rely heavily on standard API frameworks and libraries. However, learning to use those APIs is often challenging due to the fast-changing nature of API frameworks and the insufficiency of documentation and code examples. This paper introduces DroidAssist, a recommendation tool for API usages of Android mobile apps. The core of DroidAssist is HAPI, a statistical, generative model of API usages based on Hidden Markov Model. With HAPIs trained from existing mobile apps, DroidAssist could perform code completion for method calls. It can also check existing call sequences to detect and repair suspicious (i.e. unpopular) API usages."

- paper: "Mining user opinions in mobile app reviews: A keyword-based approach (t)"
  authors: "Phong Minh Vu, Tam The Nguyen, Hung Viet Pham, Tung Thanh Nguyen"
  year: "2015"
  conference: "ASE"
  abstract: "User reviews of mobile apps often contain complaints or suggestions which are valuable for app developers to improve user experience and satisfaction. However, due to the large volume and noisy-nature of those reviews, manually analyzing them for useful opinions is inherently challenging. To address this problem, we propose MARK, a keyword-based framework for semi-automated review analysis. MARK allows an analyst describing his interests in one or some mobile apps by a set of keywords. It then finds and lists the reviews most relevant to those keywords for further analysis. It can also draw the trends over time of those keywords and detect their sudden changes, which might indicate the occurrences of serious issues. To help analysts describe their interests more effectively, MARK can automatically extract keywords from raw reviews and rank them by their associations with negative reviews. In addition, based on a vector-based semantic representation of keywords, MARK can divide a large set of keywords into more cohesive subsets, or suggest keywords similar to the selected ones."

- paper: "Tool support for analyzing mobile app reviews"
  authors: "Phong Minh Vu, Hung Viet Pham, Tam The Nguyen, Tung Thanh Nguyen "
  year: "2015"
  conference: "ASE"
  abstract: "Mobile app reviews often contain useful user opinions for app developers. However, manual analysis of those reviews is challenging due to their large volume and noisynature. This paper introduces MARK, a supporting tool for review analysis of mobile apps. With MARK, an analyst can describe her interests of one or more apps via a set of keywords. MARK then lists the reviews most relevant to those keywords for further analyses. It can also draw the trends over time of the selected keywords, which might help the analyst to detect sudden changes in the related user reviews. To help the analyst describe her interests more effectively, MARK can automatically extract and rank the keywords by their associations with negative reviews, divide a large set of keywords into more cohesive subgroups, or expand a small set into a broader one."

- paper: "Similarity-based and rank-based defect prediction"
  authors: "Tung Thanh Nguyen, Tran Quang An, Vu Thanh Hai, Tu Minh Phuong"
  year: "2014"
  conference: "ATC"
  abstract: "In this paper, we explore two new approaches for software defect prediction. The similarity-based approach predicts the number of latent defects of a software module from those of modules most similar to it. The rank-based approach uses machine learning models specially trained to predict the ranks of software modules based on their actual number of latent defects. In both approaches, we use technical concerns/functionalities recovered by topic modeling techniques as features to represent software modules. Empirical evaluation with five real software systems shows that the proposed approaches outperform the traditional one and a recently introduced defect prediction method."

- paper: "Statistical learning approach for mining API usage mappings for code migration"
  authors: "Anh Tuan Nguyen, Hoan Anh Nguyen, Tung Thanh Nguyen, Tien N. Nguyen"
  year: "2014"
  conference: "ASE"
  abstract: "The same software product nowadays could appear in multiple platforms and devices. To address business needs, software companies develop a software product in a programming language and then migrate it to another one. To support that process, semi-automatic migration tools have been proposed. However, they require users to manually define the mappings between the respective APIs of the libraries used in two languages. To reduce such manual effort, we introduce StaMiner, a novel data-driven approach that statistically learns the mappings between APIs from the corpus of the corresponding client code of the APIs in two languages Java and C. Instead of using heuristics on the textual or structural similarity between APIs in two languages to map API methods and classes as in existing mining approaches, StaMiner is based on a statistical model that learns the mappings in such a corpus and provides mappings for APIs with all possible arities. Our empirical evaluation on several projects shows that StaMiner can detect API usage mappings with higher accuracy than a state-of-the-art approach. With the resulting API mappings mined by StaMiner, Java2CSharp, an existing migration tool, could achieve a higher level of accuracy."

- paper: "Statistical learning of API mappings for language migration"
  authors: "Anh Tuan Nguyen, Hoan Anh Nguyen, Tung Thanh Nguyen, Tien N Nguyen"
  year: "2014"
  conference: "ICSE"
  abstract: "The process of migrating software between languages is called language migration or code migration. To reduce manual effort in defining the rules of API mappings for code migration, we propose StaMiner, a data-driven model that statistically learns the mappings between API usages from the corpus of the corresponding methods in the client code of the APIs in two languages."

- paper: "Characterizing defect trends in software support"
  authors: "Tung Thanh Nguyen, Evelyn Duesterwald, Tim Klinger, Peter Santhanam, Tien N Nguyen"
  year: "2014"
  conference: "ICSE"
  abstract: "We present an empirical analysis of defect arrival data in the operational phase of multiple software products. We find that the shape of the defect curves is sufficiently determined by three external and readily available release cycle attributes: the product type, the license model, and the cycle time between releases. This finding provides new insights into the driving forces affecting the specifics of defect curves and opens up new opportunities for software support organizations to reduce the cost of maintaining defect arrival models for individual products. In addition, it allows the possibility of predicting the defect arrival rate of one product from another with similar known attributes."

- paper: "Migrating code with statistical machine translation"
  authors: "Anh Tuan Nguyen, Tung Thanh Nguyen, Tien N Nguyen"
  year: "2014"
  conference: "ICSE"
  abstract: "In the era of mobile computing, developers often need to migrate code written for one platform in a programming language to another language for a different platform, e.g., from Java for Android to C  for Windows Phone. The migration process is often performed manually or semi-automatically, in which developers are required to manually define translation rules and API mappings. This paper presents semSMT, an automatic tool to migrate code written in Java to C . semSMT utilizes statistical machine translation to automatically infer translation rules from existing migrated code, thus, requires no manual defining of rules. The video demonstration on semSMT can be found on YouTube at http://www.youtube.com/watch?v=aRSnl5-7vNo."

- paper: "Topic-based, time-aware bug assignment"
  authors: "Tung Thanh Nguyen, Anh Tuan Nguyen, Tien N Nguyen"
  year: "2014"
  conference: "FSE"
  abstract: "Bugs are prevalent in software systems and improving time efficiency in bug fixing is desired. We performed an analysis on 11,115 bug records of Eclipse JDT and found that bug resolution time is log-normally distributed and varies across fixers, technical topics, and bug severity levels. We then propose FixTime, a novel method for bug assignment. The key of FixTime is a topicbased, log-normal regression model for predicting defect resolution time on which FixTime is based to make fixing assignment recommendations. Preliminary results suggest that FixTime has higher prediction accuracy than existing approaches."

- paper: "Dangling references in multi-configuration and dynamic PHP-based Web applications"
  authors: "Hung Viet Nguyen, Hoan Anh Nguyen, Tung Thanh Nguyen, Anh Tuan Nguyen, Tien N Nguyen"
  year: "2013"
  conference: "ASE"
  abstract: "PHP is a dynamic language popularly used in Web development for writing server-side code to dynamically create multiple versions of client-side pages at run time for different configurations. A PHP program contains code to be executed or produced for multiple configurations/versions. That dynamism and multi-configuration nature leads to dangling references. Specifically, in the execution for a configuration, a reference to a variable or a call to a function is dangling if its corresponding declaration cannot be found. We conducted an exploratory study to confirm the existence of such dangling reference errors including dangling cross-language and embedded references in the clientside HTML/JavaScript code and in data-accessing SQL code that are embedded in scattered PHP code. Dangling references have caused run-time fatal failures and security vulnerabilities.We developed DRC, a static analysis method to detect such dangling references. DRC uses symbolic execution to collect PHP declarations/references and to approximate all versions of the generated output, and then extracts embedded declarations/references. It associates each detected declaration/reference with a conditional constraint that represents the execution paths (i.e. configurations/versions) containing that declaration/reference. It then validates references against declarations via a novel dangling reference detection algorithm. Our empirical evaluation shows that DRC detects dangling references with high accuracy. It revealed 83 yet undiscovered defects caused by dangling references.fm?id=3107656.3107707"

- paper: "A study of repetitiveness of code changes in software evolution"
  authors: "Hoan Anh Nguyen, Anh Tuan Nguyen, Tung Thanh Nguyen, Tien N Nguyen, Hridesh Rajan"
  year: "2013"
  conference: "ASE"
  abstract: "In this paper, we present a large-scale study of repetitiveness of code changes in software evolution. We collected a large data set of 2,841 Java projects, with 1.7 billion source lines of code (SLOC) at the latest revisions, 1.8 million code change revisions (0.4 million fixes), 6.2 million changed files, and 2.5 billion changed SLOCs. A change is considered repeated within or cross-project if it matches another change having occurred in the history of the project or another project, respectively. We report the following important findings. First, repetitiveness of changes could be as high as 70-100% at small sizes and decreases exponentially as size increases. Second, repetitiveness is higher and more stable in the cross-project setting than in the within-project one. Third, fixing changes repeat similarly to general changes. Importantly, learning code changes and recommending them in software evolution is beneficial with accuracy for top-1 recommendation of over 30% and top-3 of nearly 35%. Repeated fixing changes could also be useful for automatic program repair."

- paper: "Database-aware fault localization for dynamic web applications"
  authors: "Hung Viet Nguyen, Hoan Anh Nguyen, Tung Thanh Nguyen, Tien N Nguyen"
  year: "2013"
  conference: "ICSM"
  abstract: "Localizing and fixing software faults is an important maintenance task. In a dynamic Web application, localizing the faults is challenging due to its dynamic nature and the interactions between the application and databases. The faults could occur in the statements in the host program or inside the queries that are sent from the application to be executed in the database engines. This paper presents SQLook, a novel database- aware fault localization method that is able to locate output faults in PHP statements of a dynamic Web application as well as in SQL queries. In SQLook, a PHP interpreter is instrumented to execute an SQL query and to monitor the evaluation of those SQL predicates to determine if they affect the output process of individual data records. It performs row-based slicing across PHP statements and SQL queries to record the entities that are involved in the output of each data row. Our empirical evaluation shows that SQLook can achieve higher accuracy than the state- of-the-art database-aware fault localization approach."

- paper: "Output-oriented refactoring in php-based dynamic web applications"
  authors: "Hoan Anh Nguyen, Hung Viet Nguyen, Tung Thanh Nguyen, Tien N Nguyen"
  year: "2013"
  conference: "ICSM"
  abstract: "Refactoring is crucial in the development process of traditional programs as well as advanced Web applications. In a dynamic Web application, multiple versions of client code in HTML and JavaScript are dynamically generated from server-side code at run time for different usage scenarios. Toward understanding refactoring for dynamic Web code, we conducted an empirical study on several PHP-based Web applications. We found that Web developers perform a new type of refactoring that is specific to PHP-based dynamic Web code and pertain to output client-side code. After such a refactoring, the server-side code is more compact and modular with less amount of embedded and inline client-side HTML/JS code, or produces more standard-conforming client-side code. However, the corresponding output client-side code of the server code before and after the refactoring provides the same external behavior. We call it output-oriented refactoring. Our finding in the study motivates us to build WebDyn, an automatic tool for dynamicalizing refactorings. When performing on a portion of server-side code (which might contain both PHP and embedded/inline HTML/JS code), WebDyn detects the repeated and varied parts in that code portion and produces dynamic PHP code that creates the same client-side code. Our empirical evaluation on several projects showed WebDyn's accuracy in such automated refactorings."

- paper: "Lexical statistical machine translation for language migration"
  authors: "Anh Tuan Nguyen, Tung Thanh Nguyen, Tien N Nguyen"
  year: "2013"
  conference: "ESEC/FSE"
  abstract: "Prior research has shown that source code also exhibits naturalness, i.e. it is written by humans and is likely to be repetitive. The researchers also showed that the n-gram language model is useful in predicting the next token in a source file given a large corpus of existing source code. In this paper, we investigate how well statistical machine translation (SMT) models for natural languages could help in migrating source code from one programming language to another. We treat source code as a sequence of lexical tokens and apply a phrase-based SMT model on the lexemes of those tokens. Our empirical evaluation on migrating two Java projects into C showed that lexical, phrase-based SMT could achieve high lexical translation accuracy (BLEU from 81.3-82.6%). Users would have to manually edit only 11.9-15.8% of the total number of tokens in the resulting code to correct it. However, a high percentage of total translation methods (49.5-58.6%) is syntactically incorrect. Therefore, our result calls for a more program-oriented SMT model that is capable of better integrating the syntactic and semantic information of a program to support language migration."

- paper: "A statistical semantic language model for source code"
  authors: "Tung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh Nguyen, Tien N Nguyen"
  year: "2013"
  conference: "ESEC/FSE"
  abstract: "Recent research has successfully applied the statistical n-gram language model to show that source code exhibits a good level of repetition. The n-gram model is shown to have good predictability in supporting code suggestion and completion. However, the state-of-the-art n-gram approach to capture source code regularities/patterns is based only on the lexical information in a local context of the code units. To improve predictability, we introduce SLAMC, a novel statistical semantic language model for source code. It incorporates semantic information into code tokens and models the regularities/patterns of such semantic annotations, called sememes, rather than their lexemes. It combines the local context in semantic n-grams with the global technical concerns/functionality into an n-gram topic model, together with pairwise associations of program elements. Based on SLAMC, we developed a new code suggestion method, which is empirically evaluated on several projects to have relatively 18-68% higher accuracy than the state-of-the-art approach."

- paper: "DRC: A detection tool for dangling references in PHP-based web applications"
  authors: "Hung Viet Nguyen, Hoan Anh Nguyen, Tung Thanh Nguyen, Tien N Nguyen"
  year: "2013"
  conference: "ICSE"
  abstract: "PHP is a server-side language that is widely used for creating dynamic Web applications. However, as a dynamic language, PHP may induce certain programming errors that reveal themselves only at run time. A common type of error is dangling references, which occur if the referred program entities have not been declared in the current program execution. To prevent the run-time errors caused by such dangling references, we introduce Dangling Reference Checker (DRC), a novel tool to statically detect those references in the source code of PHP-based Web applications. DRC first identifies the path constraints of the program executions in which a program entity appears and then matches the path constraints of the entity's declarations and references to detect dangling ones. DRC is able to detect dangling reference errors in several real-world PHP systems with high accuracy. The video demonstration for DRC is available at http://www.youtube.com/watch?v=3Dy_AKZYhLlU4."

- paper: "Improving software quality with programming patterns"
  authors: "Tung Thanh Nguyen"
  year: "2013"
  conference: "Thesis"
  abstract: ""

- paper: "Multi-layered approach for recovering links between bug reports and fixes"
  authors: "Anh Tuan Nguyen, Tung Thanh Nguyen, Hoan Anh Nguyen, Tien N Nguyen"
  year: "2012"
  conference: "FSE"
  abstract: "The links between the bug reports in an issue-tracking system and the corresponding fixing changes in a version repository are not often recorded by developers. Such linking information is crucial for research in mining software repositories in measuring software defects and maintenance efforts. However, the state-of-the-art bug-to-fix link recovery approaches still rely much on textual matching between bug reports and commit/change logs and cannot handle well the cases where their contents are not textually similar.This paper introduces MLink, a multi-layered approach that takes into account not only textual features but also source code features of the changed code corresponding to the commit logs. It is also capable of learning the association relations between the terms in bug reports and the names of entities/components in the changed source code of the commits from the established bug-to-fix links, and uses them for link recovery between the reports and commits that do not share much similar texts. Our empirical evaluation on real-world projects shows that MLink can improve the state-of-the-art bug-to-fix link recovery methods by 11--18%, 13--17%, and 8--17% in F-score, recall, and precision, respectively.on.cfm?id=2393671"

- paper: "Detecting semantic changes in Makefile build code"
  authors: "Jafar M Al-Kofahi, Hung Viet Nguyen, Anh Tuan Nguyen, Tung Thanh Nguyen, Tien N Nguyen"
  year: "2012"
  conference: "ICSM"
  abstract: "Build code in a Makefile represents the build rules with the dependencies among the files, and how they must be built together to produce a software system. As software evolves, its build code evolves as well to accommodate necessary changes in the build process. As part of software maintenance, it is crucial to understand how the build code is changed (e.g. changes in build rules or dependencies), and to verify and validate the correctness of the build process with different build configurations. Due to Make's dynamic nature, understanding and managing the changes to Makefiles is not trivial. In this paper, we introduce a set of semantic changes to build code in Makefiles. We also develop MkDiff, a tool to detect the changes to a Makefile at the semantic level. MkDiff uses symbolic dependency graphs (SDG) to find all possible concrete rules from a Makefile, and the dependencies among them. For two SDGs built from a Makefile at two versions, it first detects changed and unchanged nodes via its SDG matching algorithm. Then, from those results, it derives the semantic changes to the Makefile. Our empirical evaluation for MkDiff showed that it can accurately detect semantic changes in Makefiles."

- paper: "Duplicate bug report detection with a combination of information retrieval and topic modeling"
  authors: "Anh Tuan Nguyen, Tung Thanh Nguyen, Tien N Nguyen, David Lo, Chengnian Sun"
  year: "2012"
  conference: "ASE"
  abstract: "Detecting duplicate bug reports helps reduce triaging efforts and save time for developers in fixing the same issues. Among several automated detection approaches, text-based information retrieval (IR) approaches have been shown to outperform others in term of both accuracy and time efficiency. However, those IR-based approaches do not detect well the duplicate reports on the same technical issues written in different descriptive terms.This paper introduces DBTM, a duplicate bug report detection approach that takes advantage of both IR-based features and topic-based features. DBTM models a bug report as a textual document describing certain technical issue(s), and models duplicate bug reports as the ones about the same technical issue(s). Trained with historical data including identified duplicate reports, it is able to learn the sets of different terms describing the same technical issues and to detect other not-yet-identified duplicate ones. Our empirical evaluation on real-world systems shows that DBTM improves the state-of-the-art approaches by up to 20% in accuracy."

- paper: "Detection of embedded code smells in dynamic web applications"
  authors: "Hung Viet Nguyen, Hoan Anh Nguyen, Tung Thanh Nguyen, Anh Tuan Nguyen, Tien N Nguyen"
  year: "2012"
  conference: "ASE"
  abstract: "In dynamic Web applications, there often exists a type of code smells, called embedded code smells, that violate important principles in software development such as software modularity and separation of concerns, resulting in much maintenance effort. Detecting and fixing those code smells is crucial yet challenging since the code with smells is embedded and generated from the server-side code. We introduce WebScent, a tool to detect such embedded code smells. WebScent first detects the smells in the generated code, and then locates them in the server-side code using the mapping between client-side code fragments and their embedding locations in the server program, which is captured during the generation of those fragments. Our empirical evaluation on real-world Web applications shows that 34%-81% of the tested server files contain embedded code smells. We also found that the source files with more embedded code smells are likely to have more defects and scattered changes, thus potentially require more maintenance effort."

- paper: "Clone management for evolving software"
  authors: "Hoan Anh Nguyen, Tung Thanh Nguyen, Nam H. Pham, Jafar Al-Kofahi, Tien N. Nguyen"
  year: "2012"
  conference: "IEEE Transactions on Software Engineering"
  abstract: "Recent research results suggest a need for code clone management. In this paper, we introduce JSync, a novel clone management tool. JSync provides two main functions to support developers in being aware of the clone relation among code fragments as software systems evolve and in making consistent changes as they create or modify cloned code. JSync represents source code and clones as (sub)trees in Abstract Syntax Trees, measures code similarity based on structural characteristic vectors, and describes code changes as tree editing scripts. The key techniques of JSync include the algorithms to compute tree editing scripts, to detect and update code clones and their groups, to analyze the changes of cloned code to validate their consistency, and to recommend relevant clone synchronization and merging. Our empirical study on several real-world systems shows that JSync is efficient and accurate in clone detection and updating, and provides the correct detection of the defects resulting from inconsistent changes to clones and the correct recommendations for change propagation across cloned code."

- paper: "GraPacc: a graph-based pattern-oriented, context-sensitive code completion tool"
  authors: "Anh Tuan Nguyen, Hoan Anh Nguyen, Tung Thanh Nguyen, Tien N Nguyen"
  year: "2012"
  conference: "ICSE"
  abstract: "Code completion tool plays an important role in daily development activities. It helps developers by auto-completing tedious and detailed code during an editing session. However, existing code completion tools are limited to recommending only context-free code templates and a single method call of the variable under editing. We introduce GraPacc, an advanced, context-sensitive code completion tool that is based on frequent API usage patterns. It extracts the context-sensitive features from the code under editing, for example, the API elements on focus and the current editing point, and their relations to other code elements. It then ranks the relevant API usage patterns and auto-completes the current code with the proper elements according to the chosen pattern."

- paper: "Inferring developer expertise through defect analysis"
  authors: "Tung Thanh Nguyen, Tien N Nguyen, Evelyn Duesterwald, Tim Klinger, Peter Santhanam"
  year: "2012"
  conference: "ICSE"
  abstract: "Fixing defects is an essential software development activity. For commercial software vendors, the time to repair defects in deployed business-critical software products or applications is a key quality metric for sustained customer satisfaction. In this paper, we report on the analysis of about 1,500 defect records from an IBM middle-ware product collected over a five-year period. The analysis includes a characterization of each repaired defect by topic and a ranking of developers by inferred expertise on each topic. We find clear evidence that defect resolution time is strongly influenced by the specific developer and his/her expertise in the defect's topic. To validate our approach, we conducted interviews with the productâ��s manager who provided us with his own ranking of developer expertise for comparison. We argue that our automated developer expertise ranking can be beneficial in the planning of a software project and is applicable beyond software support in the other phases of the software lifecycle."

- paper: "BabelRef: detection and renaming tool for cross-language program entities in dynamic web applications"
  authors: "Hung Viet Nguyen, Hoan Anh Nguyen, Tung Thanh Nguyen, Tien N Nguyen"
  year: "2012"
  conference: "ICSE"
  abstract: "In a dynamic web application, client-side code is often dynamically generated from server-side code. Client-side program entities such as HTML presentation elements and Javascript functions/variables are embedded within server-side string literals or variables' values. However, existing tools for code maintenance such as automatic renaming support only work for program entities in a single language on either the server side or the client side. In this paper, we introduce BabelRef, a novel tool that is able to automatically identify and rename client-side program entities and their references that are embedded within server-side code."

- paper: "Graph-based pattern-oriented, context-sensitive source code completion"
  authors: "Anh Tuan Nguyen, Tung Thanh Nguyen, Hoan Anh Nguyen, Ahmed Tamrawi, Hung Viet Nguyen, Jafar Al-Kofahi, Tien N Nguyen"
  year: "2012"
  conference: "ICSE"
  abstract: "Code completion helps improve developers' programming productivity. However, the current support for code completion is limited to context-free code templates or a single method call of the variable on focus. Using software libraries for development, developers often repeat API usages for certain tasks. Thus, a code completion tool could make use of API usage patterns. In this paper, we introduce GraPacc, a graph-based, pattern-oriented, context-sensitive code completion approach that is based on a database of such patterns. GraPacc represents and manages the API usage patterns of multiple variables, methods, and control structures via graph-based models. It extracts the context-sensitive features from the code under editing, e.g. the API elements on focus and their relations to other code elements. Those features are used to search and rank the patterns that are most fitted with the current code. When a pattern is selected, the current code will be completed via a novel graph-based code completion algorithm. Empirical evaluation on several real-world systems shows that GraPacc has a high level of accuracy in code completion."

- paper: "idiff: Interaction-based program differencing tool"
  authors: "Hoan Anh Nguyen, Tung Thanh Nguyen, Hung Viet Nguyen, Tien N Nguyen"
  year: "2011"
  conference: "ASE"
  abstract: "When a software system evolves, its program entities such as classes/methods are also changed. System comprehension, maintenance, and other tasks require the detection of the changed entities between two versions. However, existing differencing tools are file-based and cannot handle well the common cases in which the methods/classes are reordered/moved or even renamed/modified. Moreover, many tools show the program changes at the text line level. In this demo, we present iDiff, a program differencing tool that is able to display the changes to classes/methods between two versions and to track the corresponding classes/methods even they were reordered/moved/renamed and/or modified. The key idea is that during software evolution, an entity could change its location, name, order, and even its internal implementation. However, its interaction with other entities would be more stable. iDiff represents a system at a version as an attributed graph, in which the nodes represent program entities, the edges represent the interactions between the nodes. Entities between two versions are matched via an incremental matching algorithm, which takes into account the similarity of interactions for matching. The differences of two versions of the entire system including its program entities are detected based on the matched entities."

- paper: "A topic-based approach for narrowing the search space of buggy files from a bug report"
  authors: "Anh Tuan Nguyen, Tung Thanh Nguyen, Jafar Al-Kofahi, Hung Viet Nguyen, Tien N Nguyen"
  year: "2011"
  conference: "ASE"
  abstract: "Locating buggy code is a time-consuming task in software development. Given a new bug report, developers must search through a large number of files in a project to locate buggy code. We propose BugScout, an automated approach to help developers reduce such efforts by narrowing the search space of buggy files when they are assigned to address a bug report. BugScout assumes that the textual contents of a bug report and that of its corresponding source code share some technical aspects of the system which can be used for locating buggy source files given a new bug report. We develop a specialized topic model that represents those technical aspects as topics in the textual contents of bug reports and source files, and correlates bug reports and corresponding buggy files via their shared topics. Our evaluation shows that BugScout can recommend buggy files correctly up to 45% of the cases with a recommended ranked list of 10 files."

- paper: "Auto-locating and fix-propagating for HTML validation errors to PHP server-side code"
  authors: "Hung Viet Nguyen, Hoan Anh Nguyen, Tung Thanh Nguyen, Tien N Nguyen"
  year: "2011"
  conference: "ASE"
  abstract: "Checking/correcting HTML validation errors in Web pages is helpful for Web developers in finding/fixing bugs. However, existing validating/fixing tools work well only on static HTML pages and do not help fix the corresponding server code if validation errors are found in HTML pages, due to several challenges with dynamically generated pages in Web development. We propose PhpSync, a novel automatic locating/fixing tool for HTML validation errors in PHP-based Web applications. Given an HTML page produced by a server-side PHP program, PhpSync uses Tidy, an HTML validating/correcting tool to find the validation errors in that HTML page. If errors are detected, it leverages the fixes from Tidy in the given HTML page and propagates them to the corresponding location(s) in PHP code. Our core solutions include 1) a symbolic execution algorithm on the given PHP program to produce a single tree-based model, called D-model, which approximately represents its possible client page outputs, 2) an algorithm mapping any text in the given HTML page to the text(s) in the node(s) of the D-model and then to the PHP code, and 3) a fix-propagating algorithm from the fixes in the HTML page to the PHP code via the D-model and the mapping algorithm. Our empirical evaluation shows that on average, PhpSync achieves 96.7% accuracy in locating the corresponding locations in PHP code from client pages, and 95% accuracy in propagating the fixes to the server-side code."

- paper: "Fuzzy set and cache-based approach for bug triaging"
  authors: "Ahmed Tamrawi, Tung Thanh Nguyen, Jafar M Al-Kofahi, Tien N Nguyen"
  year: "2011"
  conference: ESEC/FSE
  abstract: "Bug triaging aims to assign a bug to the most appropriate fixer. That task is crucial in reducing time and efforts in a bug fixing process. In this paper, we propose Bugzie, a novel approach for automatic bug triaging based on fuzzy set and cache-based modeling of the bug-fixing expertise of developers. Bugzie considers a software system to have multiple technical aspects, each of which is associated with technical terms. For each technical term, it uses a fuzzy set to represent the developers who are capable/competent of fixing the bugs relevant to the corresponding aspect. The fixing correlation of a developer toward a technical term is represented by his/her membership score toward the corresponding fuzzy set. The score is calculated based on the bug reports that (s)he has fixed, and is updated as the newly fixed bug reports are available. For a new bug report, Bugzie combines the fuzzy sets corresponding to its terms and ranks the developers based on their membership scores toward that combined fuzzy set to find the most capable fixers. Our empirical results show that Bugzie achieves significantly higher accuracy and time efficiency than existing state-of-the-art approaches."

- paper: "Topic-based defect prediction (nier track)"
  authors: "Tung Thanh Nguyen, Tien N Nguyen, Tu Minh Phuong"
  year: "2011"
  conference: "ICSE"
  abstract: "Defects are unavoidable in software development and fixing them is costly and resource-intensive. To build defect prediction models, researchers have investigated a number of factors related to the defect-proneness of source code, such as code complexity, change complexity, or socio-technical factors. In this paper, we propose a new approach that emphasizes on technical concerns/functionality of a system. In our approach, a software system is viewed as a collection of software artifacts that describe different technical concerns/-aspects. Those concerns are assumed to have different levels of defect-proneness, thus, cause different levels of defectproneness to the relevant software artifacts. We use topic modeling to measure the concerns in source code, and use them as the input for machine learning-based defect prediction models. Preliminary result on Eclipse JDT shows that the topic-based metrics have high correlation to the number of bugs (defect-proneness), and our topic-based defect prediction has better predictive performance than existing state-of-the-art approaches."

- paper: "Aspect recommendation for evolving software"
  authors: "Tung Thanh Nguyen, Hung Viet Nguyen, Hoan Anh Nguyen, Tien N Nguyen"
  year: "2011"
  conference: "ICSE"
  abstract: "Cross-cutting concerns are unavoidable and create difficulties in the development and maintenance of large-scale systems. In this paper, we present a novel approach that identifies certain groups of code units that potentially share some cross-cutting concerns and recommends them for creating and updating aspects. Those code units, called concern peers, are detected based on their similar interactions (similar calling relations in similar contexts, either internally or externally). The recommendation is applicable to both the aspectization of non-aspect-oriented programs (i.e. for aspect creation), and the evolution of aspect-oriented programs (i.e. for aspect updating). The empirical evaluation on several real-world software systems shows that our approach is scalable and provides useful recommendations."

- paper: "A graph-based approach to API usage adaptation"
  authors: "Hoan Anh Nguyen, Tung Thanh Nguyen, Gary Wilson Jr, Anh Tuan Nguyen, Miryung Kim, Tien N Nguyen"
  year: "2010"
  conference: "OOPSLA"
  abstract: "Reusing existing library components is essential for reducing the cost of software development and maintenance. When library components evolve to accommodate new feature requests, to fix bugs, or to meet new standards, the clients of software libraries often need to make corresponding changes to correctly use the updated libraries. Existing API usage adaptation techniques support simple adaptation such as replacing the target of calls to a deprecated API, however, cannot handle complex adaptations such as creating a new object to be passed to a different API method, or adding an exception handling logic that surrounds the updated API method calls.This paper presents LIBSYNC that guides developers in adapting API usage code by learning complex API usage adaptation patterns from other clients that already migrated to a new library version (and also from the API usages within the library's test code). LIBSYNC uses several graph-based techniques (1) to identify changes to API declarations by comparing two library versions, (2) to extract associated API usage skeletons before and after library migration, and (3) to compare the extracted API usage skeletons to recover API usage adaptation patterns. Using the learned adaptation patterns, LIBSYNC recommends the locations and edit operations for adapting API usages. The evaluation of LIBSYNC on real-world software systems shows that it is highly correct and useful with a precision of 100% and a recall of 91%."

- paper: "Fuzzy set approach for automatic tagging in evolving software"
  authors: "Jafar M Al-Kofahi, Ahmed Tamrawi, Tung Thanh Nguyen, Hoan Anh Nguyen, Tien N Nguyen"
  year: "2010"
  conference: "ICSM"
  abstract: "Software tagging has been shown to be an efficient, lightweight social computing mechanism to improve different social and technical aspects of software development. Despite the importance of tags, there exists limited support for automatic tagging for software artifacts, especially during the evolutionary process of software development. We conducted an empirical study on IBM Jazz's repository and found that there are several missing tags in artifacts and more precise tags are desirable. This paper introduces a novel, accurate, automatic tagging recommendation tool that is able to take into account users' feedbacks on tags, and is very efficient in coping with software evolution. The core technique is an automatic tagging algorithm that is based on fuzzy set theory. Our empirical evaluation on the real-world IBM Jazz project shows the usefulness and accuracy of our approach and tool."

- paper: "Detecting recurring and similar software vulnerabilities"
  authors: "Nam H Pham, Tung Thanh Nguyen, Hoan Anh Nguyen, Xinying Wang, Anh Tuan Nguyen, Tien N Nguyen"
  year: "2010"
  conference: "ICSE"
  abstract: "New software security vulnerabilities are discovered on almost daily basis and it is vital to be able to identify and resolve them as early as possible. Fortunately, many software vulnerabilities are recurring or very similar, thus, one could effectively detect and fix a vulnerability in a system by consulting the similar vulnerabilities and fixes from other systems. In this paper, we propose, SecureSync, an automatic approach to detect and provide suggested resolutions for recurring software vulnerabilities on multiple systems sharing/using similar code or API libraries. The core of SecureSync includes a usage model and a mapping algorithm for matching vulnerable code across different systems, a model for the comparison of vulnerability reports, and a tracing technique from a report to corresponding source code. Our preliminary evaluation with case studies showed the potential usefulness of SecureSync."

- paper: "Recurring bug fixes in object-oriented programs"
  authors: "Tung Thanh Nguyen, Hoan Anh Nguyen, Nam H Pham, Jafar Al-Kofahi, Tien N Nguyen"
  year: "2010"
  conference: "ICSE"
  abstract: "Previous research confirms the existence of recurring bug fixes in software systems. Analyzing such fixes manually, we found that a large percentage of them occurs in code peers, the classes/methods having the similar roles in the systems, such as providing similar functions and/or participating in similar object interactions. Based on graph-based representation of object usages, we have developed several techniques to identify code peers, recognize recurring bug fixes, and recommend changes for code units from the bug fixes of their peers. The empirical evaluation on several open-source projects shows that our prototype, FixWizard, is able to identify recurring bug fixes and provide fixing recommendations with acceptable accuracy."

- paper: "Operation-based, fine-grained version control model for tree-based representation"
  authors: "Tung Thanh Nguyen, Hoan Anh Nguyen, Nam H Pham, Tien N Nguyen"
  year: "2010"
  conference: "FASE"
  abstract: "Existing version control systems are often based on text line-oriented models for change representation, which do not facilitate software developers in understanding code evolution. Other advanced change representation models that encompass more program semantics and structures are still not quite practical due to their high computational complexity. This paper presents OperV, a novel operation-based version control model that is able to support both coarse and fine levels of granularity in program source code. In OperV, a software system is represented by a project tree whose nodes represent all program entities, such as packages, classes, methods, etc. The changes of the system are represented via edit operations on the tree. OperV also provides the algorithms to differ, store, and retrieve the versions of such entities. These algorithms are based on the mapping of the nodes between versions of the project tree. This mapping technique uses 1) divide-and-conquer technique to map coarse- and fine-grained entities separately, 2) unchanged text regions to map unchanged leaf nodes, and 3) structure-based similarity of the sub-trees to map their root nodes bottom-up and then top-down. The empirical evaluation of OperV has shown that it is scalable, efficient, and could be useful in understanding program evolution."

- paper: "Detection of recurring software vulnerabilities"
  authors: Nam H. Pham, Tung T. Nguyen, Hoan A Nguyen, Tien N. Nguyen
  year: "2010"
  conference: "ASE"
  abstract: "Software security vulnerabilities are discovered on an almost daily basis and have caused substantial damage. Aiming at supporting early detection and resolution for them, we have conducted an empirical study on thousands of vulnerabilities and found that many of them are recurring due to software reuse. Based on the knowledge gained from the study, we developed SecureSync, an automatic tool to detect recurring software vulnerabilities on the systems that reuse source code or libraries. The core of SecureSync includes two techniques to represent and compute the similarity of vulnerable code across different systems. The evaluation for 60 vulnerabilities on 176 releases of 119 open-source software systems shows that SecureSync is able to detect recurring vulnerabilities with high accuracy and to identify 90 releases having potentially vulnerable code that are not reported or fixed yet, even in mature systems. A couple of cases were actually confirmed by their developers."

- paper: "Clone-aware configuration management"
  authors: "Tung Thanh Nguyen, Hoan Anh Nguyen, Nam H Pham, Jafar M Al-Kofahi, Tien N Nguyen"
  year: "2009"
  conference: "ASE"
  abstract: "Recent research results show several benefits of the management of code clones. In this paper, we introduce Clever, a novel clone-aware software configuration management (SCM) system. In addition to traditional SCM functionality, Clever provides clone management support, including clone detection and update, clone change management, clone consistency validating, clone synchronizing, and clone merging. Clever represents source code and clones as (sub)trees in Abstract Syntax Trees (ASTs), measures code similarity based on structural characteristic vectors, and describes code changes as tree editing scripts. The key techniques of Clever include the algorithms to compute tree editing scripts; to detect and update code clones and their groups; and to analyze the changes of cloned code to validate their consistency and recommend the relevant synchronization. Our empirical study on many real-world programs shows that Clever is highly efficient and accurate in clone detection and updating, and provides useful analysis of clone changes."

- paper: "Scalable and incremental clone detection for evolving software"
  authors: "Tung Thanh Nguyen, Hoan Anh Nguyen, Jafar M Al-Kofahi, Nam H Pham, Tien N Nguyen"
  year: "2009"
  conference: "ICSM"
  abstract: "Code clone management has been shown to have several benefits for software developers. When source code evolves, clone management requires a mechanism to efficiently and incrementally detect code clones in the new revision. This paper introduces an incremental clone detection tool, called ClemanX. Our tool represents code fragments as subtrees of Abstract Syntax Trees (ASTs), measures their similarity levels based on their characteristic vectors of structural features, and solves the task of incrementally detecting similar code as an incremental distance-based clustering problem. Our empirical evaluation on large-scale software projects shows the usefulness and good performance of ClemanX."

- paper: "Graph-based mining of multiple object usage patterns"
  authors: "Tung Thanh Nguyen, Hoan Anh Nguyen, Nam H Pham, Jafar M Al-Kofahi, Tien N Nguyen"
  year: "2009"
  conference: ESEC/FSE
  abstract: "The interplay of multiple objects in object-oriented programming often follows specific protocols, for example certain orders of method calls and/or control structure constraints among them that are parts of the intended object usages. Unfortunately, the information is not always documented. That creates long learning curve, and importantly, leads to subtle problems due to the misuse of objects.In this paper, we propose GrouMiner, a novel graph-based approach for mining the usage patterns of one or multiple objects. GrouMiner approach includes a graph-based representation for multiple object usages, a pattern mining algorithm, and an anomaly detection technique that are efficient, accurate, and resilient to software changes. Our experiments on several real-world programs show that our prototype is able to find useful usage patterns with multiple objects and control structures, and to translate them into user-friendly code skeletons to assist developers in programming. It could also detect the usage anomalies that caused yet undiscovered defects and code smells in those programs."

- paper: "ClemanX: Incremental clone detection tool for evolving software"
  authors: "Tung Thanh Nguyen, Hoan Anh Nguyen, Nam H Pham, Jafar M Al-Kofahi, Tien N Nguyen"
  year: "2009"
  conference: "ICSE"
  abstract: "Recent research results have shown more benefits of the management of code clones, rather than detecting and removing them. However, existing clone management approaches are still unsatisfactory, and either incomplete or inefficient, due to the lack of incremental clone detection tool supports. In this paper, we introduce such an incremental clone detection tool, called ClemanX. Our empirical evaluation on real-world software projects shows that ClemanX is highly efficient, complete, precise, and is capable of working incrementally when the code changes."

- paper: "Complete and accurate clone detection in graph-based models"
  authors: "Nam H Pham, Hoan Anh Nguyen, Tung Thanh Nguyen, Jafar M Al-Kofahi, Tien N Nguyen"
  year: "2009"
  conference: "ICSE"
  abstract: "Model-Driven Engineering (MDE) has become an important development framework for many large-scale software. Previous research has reported that as in traditional code-based development, cloning also occurs in MDE. However, there has been little work on clone detection in models with the limitations on detection precision and completeness. This paper presents ModelCD, a novel clone detection tool for Matlab/Simulink models, that is able to efficiently and accurately detect both exactly matched and approximate model clones. The core of ModelCD is two novel graph-based clone detection algorithms that are able to systematically and incrementally discover clones with a high degree of completeness, accuracy, and scalability. We have conducted an empirical evaluation with various experimental studies on many real-world systems to demonstrate the usefulness of our approach and to compare the performance of ModelCD with existing tools."

- paper: "Accurate and efficient structural characteristic feature extraction for clone detection"
  authors: "Hoan Anh Nguyen, Tung Thanh Nguyen, Nam H Pham, Jafar M Al-Kofahi, Tien N Nguyen"
  year: "2009"
  conference: "FASE"
  abstract: "Structure-oriented approaches in clone detection have become popular in both code-based and model-based clone detection. However, existing methods for capturing structural information in software artifacts are either too computationally expensive to be efficient or too light-weight to be accurate in clone detection. In this paper, we present Exas, an accurate and efficient structural characteristic feature extraction approach that better approximates and captures the structure within the fragments of artifacts. Exas structural features are the sequences of labels and numbers built from nodes, edges, and paths of various lengths of a graph-based representation. A fragment is characterized by a structural characteristic vector of the occurrence counts of those features. We have applied Exas in building two clone detection tools for source code and models. Our analytic study and empirical evaluation on open-source software show that Exas and its algorithm for computing the characteristic vectors are highly accurate and efficient in clone detection."

- paper: "Cleman: Comprehensive clone group evolution management"
  authors: "Tung Thanh Nguyen, Hoan Anh Nguyen, Nam H Pham, Jafar M Al-Kofahi, Tien N Nguyen"
  year: "2008"
  conference: "ASE"
  abstract: "Recent research results have shown more benefits of the management of code clones, rather than detecting and removing them. However, existing management approaches for code clone group evolution are still ad hoc, unsatisfactory, and limited. In this paper, we introduce a novel method for comprehensive code clone group management in evolving software. The core of our method is Cleman, an algorithmic framework that allows for a systematic construction of efficient and accurate clone group management tools. Clone group management is rigorously formulated by a formal model, which provides the foundation for Cleman framework. We use Cleman framework to build a clone group management tool that is able to detect high-quality clone groups and efficiently manage them when the software evolves. We also conduct an empirical evaluation on real-world systems to show the flexibility of Cleman framework and the efficiency, completeness, and incremental updatability of our tool."